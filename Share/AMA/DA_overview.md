## DA AMA

### JasonWan：

---

大家好，我是JasonWan，是Arbitrum的集成工程师，目前负责Arbitrum亚太区的中文开发者社区，今天很高兴来和大家一起探讨数据可用性（D/A）的话题，D/A的话题最近被广泛提及，今天我们邀请了业内的几位嘉宾一起来探讨一下什么是D/A以及D/A有什么作用和好处。

### Nina：
大家好，我是Nina，是Arbitrum亚太区的生态负责人，各位嘉宾中我与潘老师比较熟悉，今天主要是和各位开发者一起来学习D/A。

### 潘致雄：

---

谢谢Jason，大家好，我是潘致雄，平时比较关注区块链的话题。之前很少讨论D/A，我对D/A的理解也不一定深，如果有什么问题的话，希望和大家一起来探讨。

### QiZhou：

---

大家好，我是周齐，是Web3Q和QuarkChain的创始人，一直以来专注于web3的基础设施，包括分片和数据可用性。最近和以太坊团队在D/A的一些标准(如EIP-4844)等方面有所交流，希望能和大家一起来聊聊数据可用性是怎么一回事，谢谢大家！

### cyberorange：

---

大家好，我是cyberorange，现在在Unipass做研究员，Unipass是一个低门槛的智能钱包，大家可以用邮箱管理和控制自己的资产，我平时也会利用空余时间去研究一些以太坊的技术进展,曾在dapplearning也做过一次D/A的分享。

### 话题一：

现在D/A的概念很火，请cyberorange来解释一下D/A的概念吧！

### cyberorange：

---

要讲什么是D/A，我们可以先回顾一下以太坊之前的扩容方案，以太坊之前是使用分片进行扩容的，所谓的分片，就是把资源拆分开到多台机器上去执行，有执行分片，状态分片，网络分片等类型，它们都代表着某一资源的拆分，但是后来发现，这一条路径是很难走的，它的设计过于复杂。

然后就出现了Plasma，但是plasma也存在问题，在挑战期可能会出现数据扣留，最后大家在plasma上面退了一步，设计出了rollup，rollup实现了将执行层从以太坊剥离，这么做虽然可以起到一定的扩容，但是扩容效果依然无法实现大规模采用，因为rollup需要把所有的交易信息以calldata的形式传到Layer1上，无论是压缩交易或者是zk-rollup的不将签名放到链上，交易花销终究都是和交易大小成正比，而以太坊本身的数据吞吐能力是较低的，这样就使得rollup扩容在当前的状态下天花板十分明显。但是又不能不提交数据，因为像op-rollup如果没有交易数据就无法验证一笔交易的正确，也无法对错误交易发起挑战。而zk-rollup如果没有数据，则无法构造当前状态，定序器就可以审查你的交易，比如在状态树里面删除了你的账户，虽然它无法构造错误的交易，但是你的钱就永远无法取出，所以说，以太坊除了在Rollup方向进行扩容，还需要在数据方面扩容，即D/A 。

它是一种保证数据可用性的能力，并没有计算能力。你可以把它理解为一个告示板，它可以在告示板上面贴数据，任何对数据感兴趣的，都可以去看，去读取数据。然后你根据内容重建某个应用上你感兴趣的状态，所以说D/A最重要的是，无需许可的重建状态。很多人会把D/A 和存储混淆，但是其实是不一样的，D/A并不保证存储数据一直有效，只是作为一个告示版，像EIP-4844，它提供了1M的Blobs空间供数据存储使用，但是会在一个月以后删除，所以他们的目的是不一样的。目前D/A所采用的技术主要是纠删码（RS纠删码）和数据采样（DAS），这是在18年的某篇论文中就定下来的技术方案。通过纠删码你可以做到某个节点只存一部分数据，然后任意一定比例的节点在线都可以把数据完整恢复出来。比如你有100份数据，通过纠删码编码300份，分散到300个节点上面，只要任意100个节点在线你就能把数据完整恢复出来。采样可以让一个节点不下载数据，只需要抽取部分数据片段就以极高的概率确定这个数据在当前网络是否可用。大概估计一下你只要抽十几个数据片就可以接近100%（99.9999……%）的概率来保证这个数据是可用的。

回过头来看，当你把D/A和rollup结合，就会发现实现了最初的资源拆分，D/A拆分了存储，Rollup拆分了执行和计算，最终的效果是类似的，但是设计哲学是不一样的。

### JasonWan：

---

cyberorange的介绍十分详细，不但介绍了什么是D/A，还介绍了D/A的诞生的发展，也提到了plasma技术和EIP-4844所要使用到的技术，非常感谢cyberorange！第一个话题结束，相信大家已经有所了解，接下来我们聊第二个话题。

### 话题二：

---

D/A由于需要存放数据，目前对于D/A的存储有很多方法，请嘉宾来聊聊对D/A的一些看法。

### QiZhou：

---

的确如cyberorange给大家讲的D/A的历史和发展一样，一开始D/A的数据路线是从以太坊对Rollup的需求开始的，将以太坊作为Rollup未来的D/A层是未来以太坊的技术方向。在这方面，EIP-4844的设计目标是让Beacon Chain Node（信标链节点）能够携带1-2M的Blobs的区块大小，2M除以12s的时间大概就能就能算出吞吐量。The Merge之后想要解决的一个问题是把效率提高，使用到的一个核心的技术方向就是DAS，它的思想就是只用采样很小的一部分数据，比如说整个数据的万分之几，在概率层面上可以确定数据是在网络中被存储和恢复的，这里有个前提就是其他所有的Validator都得做类似的随机采样，是诚实的验证节点，这样才能保证数据可用，才可以通过很小的采样数据确保数据在全网可用，不用去下载全部数据验证。通过这个技术的设计，可以达到32M的存储来提高吞吐速率，32M除以12s大概可以得到2.6M/s左右的吞吐量。在这里面，还有很多D/A的选择，在最早的时候，Vitalik也提过使用类似BCH那样的大区块技术来做D/A层，BCH和不同的分叉版本计划将原来比特币1M的区块扩大到32M甚至128M的大小，但是相对于比特币的出块时间来看速率也还是比较低的。它的机制也是采用P2P广播机制，这套机制的技术是比较落后的，需要广播数据到全网来下载区块数据，以太坊意识到在这方面的问题，所以下决心去使用自己的方案来解决D/A的问题。

除了上传之外还有一个保存的问题，现在有一些保存的方案，但是现在来说还不是最重要的重点。除此之外，还有celestia、avail这样的方案，据我所知，Arbitrum也在做一些Layer2的D/A方案。

### JasonWan：

---

感谢周齐老师详细的从D/A的关键数据参数如出块时间，区块大小等方面来对D/A进行了解读，请潘老师来发言谈谈看法。

### 潘致雄：

---

谢谢Jason,我做一些补充，角度不太一样，我觉得D/A是从blocks base衍生出的话题，我觉得是行业对区块链的理解逐渐在加深，区块链的集成度是比较高的，比如说包含共识，计算，存储，P2P网络等，一个大的趋势是大家都想解耦这些功能，把它分为某些小的部分来单独的优化和处理，所以以太坊才会在未来把D/A单独的做一层，独立于共识，虚拟机的部分，按照现在来看，D/A主要是通过区块空间来实现，但是未来会有区块链来专门提供D/A，这是对于layer1来说，但是对于很多Layer2，会采用自建D/A的方式。所以说，我觉得整体的路线是从泛用型区块链变成了专用型的D/A解决方案。在未来，D/A会成为垂直型的功能，大家会对它进行探索，无论是它的吞吐量，有效期，性能，采样安全等，都会做的更专业。

### JasonWant：

---

潘老师从模块化区块链的角度对D/A进行了补充，又从垂直领域对D/A进行了介绍，感谢潘老师！现在我们进入第三个问题。

### 话题三：

现在D/A的分类有很多，比如侧链，新公链和rollup都有自己的D/A方案，也提到了各自方案的优点，请嘉宾来聊聊对这些D/A方案的优缺点。

### cyberorange：

---

目前确实有挺多的D/A的尝试，事实上，所以的layer1都承载了D/A的功能，这里我们只谈单独做D/A的方案。

目前看到的D/A方案都是和对应的需求有关，比如说做游戏这类弱金融和弱审查的应用，像Starkware和Arbitrum都会采用委员会做节点的方式来做D/A ，它的去中心化程度和抗审查能力会比较低，使用这些D/A方案的一般做资产结算不太适合，但是适合用来做社交，链游这类对结算要求较低的应用。

然后就是celestia，他们创始人是最早发了DAS的那篇论文，还有Polygon Avail，他们的解决方案比较类似，区别在于celestia使用的是纠删码和错误编码证明，Avail使用的是Danksharding的KZG，他们的主要区别是：错误编码证明在于出块人出块以后这个块可能是错误的，然后其他人收到这个块以后可以重新编码，如果编码出来不一样，他们可以提供你编码错误的声明，在全网广播。Avail使用的KZG在于不使用错误编码证明，因为它直接约束了正确的编码。这两个区别就比较类似op-rollup和zk-rollup的区别。

当然了，layer2项目翘首以盼的是以太坊推出自己的D/A ，以前以太坊的路线是推出64个分片，但是现在采用了Danksharding方案，虽然说设计更为简化，但是在短期也无法推出，因为涉及到一些暂时无法很好解决的问题，比如说PBS，但是呢，以太坊基金会也准备了EIP-4844和EIP-4488，这是Danksharding的前置步骤Proto-Danksharding，为rollup降低了费用，先进行扩容解决燃眉之急。我觉得The Merge以后的最近一两次硬分叉就会把这两个EIP加入进去。他们的区别是用链原生的D/A安全假设更低、更优雅，抗审查能力也会更强。但如果是结算层用以太坊，D/A用celestia或者Avail的话，以太坊是无法对上面的D/A进行采样的，所以说它可能会带来一些安全模型上面的问题，比如说，celestia的验证人想欺骗轻节点，他们可以出一个块，放到以太坊上，然后对轻节点在网络上进行日蚀攻击，他就有可能欺骗某些应用，而轻节点是没法提供挑战或其他方式去证明它欺诈了你的。

总结一下，刚刚说了三种类型的D/A：原生D/A，celestia，Avail这样的独立D/A，委员会D/A。他们的去中心化程度和抗审查能力在逐步降低，实现难度也是越来越低，所以我们可能会先用到委员会D/A，像Arbitrum Nova,然后再是Celestia，最后才会使用到以太坊的原生D/A。

### JasonWan：

---

非常感谢cyberorange从D/A使用情况和实现难度来进行分析，接下来请潘老师来分享一下。

### 潘致雄：

---

cyberorange所提到的有一点，我可以进行补充，像celestia、polygon Avail，还有以太坊未来的Proto-Danksharding，我记得他们最核心的区别在于，celestia使用的是欺诈证明，而另外两个方案使用的是KZG，这是最核心的差异之一。对于现行的D/A方案，我觉得侧链和新兴的以太坊范式的区块链，他们如果想做D/A的话，最核心的缺点是共识得从零开始，这是难度最大的地方。但是D/A技术层面选择上和以太坊是没有太大的差别的。现在以太坊还有很多的Layer2，想要跳开以太坊，将数据放在自己的服务上，我觉得这种方案比较折中且能兼顾安全性和效率，如果出现了问题，这些方案能够回退到以太坊来提供安全保障，Arbitrum的Nova链在委员会失效以后也是能够回退到Rollup模式的。如果发现committee（委员会）里面的某些验证者有问题了，那可以及时退出确保安全性，这样的方案相比侧链的方案也会更安全些。由于以太坊最初并不是为D/A设计的，当时是一个泛用型解决方案，如果说现在以以太坊作为D/A的话，它的成本是比较高的，没有考虑到很多特殊的场景，也没有考虑到数据的有效期的问题，如果能在上海分叉或者The Merge以后的升级上线Proto-Danksharding的话，还是能够给Layer2的专用D/A层带来新的方案和设计思路的。

另外一个就是专为D/A设计的新公链了，包括celestia，对于Avail我不知道是否会脱离开以太坊独立，应该也会，这些公链的D/A层，优缺点还不是很明确，但是或许可以在D/A技术的选择上诞生更有意思的新技术，当然他们也推动了DAS，KZG这些技术的进步。但是从共识上来说，因为是比较新的方案，所以不如以太坊来做D/A层更安全。

### JasonWan：

---

感谢潘老师从技术生态等方面分析了D/A ，总结的很全面。请周齐老师分享一下看法。

### QiZhou：

---

大家刚刚讨论的都比较细，从侧链，新公链，以太坊本身等的角度进行了剖析，我从其他不同的方面来聊聊我对D/A的分析。

我觉得在很多方案里面，把大量的Data Blobs上传过程的设计是不一样的，对安全性，去中心化，节点数目的要求是完全不一样的。

首先最简单的方式是大家都去广播区块，比如比特币，大区块的比特币，包括侧链和celestia都是使用Tendermint引擎作为共识引擎出块，整个思路还是直接进行广播，这样的话会存在一个缺点就是受制于性能上全节点的数目尤其是出块节点的数目不能太多，因为太多的话会形成网络延迟问题。在这方面，我看到两个方向来解决这个问题，一个是以太坊的DAS技术和PBS技术，核心的思想是所有的验证者（可以很多，上万个）不需要使用P2P网络进行数据的广播和上传，这个方面有很多的技术挑战，包括对网络架构的调整，因为会有很多的子网络不是像现在的D/A方案都是通过P2P网络进行数据广播，它是通过数据可用性采样（DAS）和子网的方式来让所有的验证者节点和普通节点在不下载全部数据的情况下确认数据被完整的上传到了整个网络里，这个是比较有意思的方案，它的去中心化程度和上传的数据都比较高。

另外的话我们可以看到aptos和sui这样的新公链方案，使用一些类似传统的P2P网络，但是会更加高效的利用mempool的信息去提高他们整个网络的上传速率，我看到最新的上传速率在10M每秒左右。这个是从网络的上传角度来说的一些比较有意思的点。

### JasonWan：

---

感谢周齐老师的补充，给到了数据上传相对准确的数据，谢谢各位嘉宾。对于D/A，侧链，rollups，新公链都有自己的D/A方案，我再介绍一下Arbitrum的D/A方案。

Arbitrum One的D/A 是rollup的方案，数据的可用性在以太坊主网，calldata的数据会上传到Inbox合约。最近大家看到我们发布了Arbitrum Nova链，Nova链采用的就是刚刚cyberorange所提到的委员会D/A方案。虽然我们采用委员会的方案，但并不是说我们把系统的安全性都交给了委员会，即使委员会出现了问题，我们也能使系统保持安全状态，也就是Fallback to Rollup。如果网络都处于安全状态，没有委员会成员作恶的话，我们就会采用委员会来保存DA。如果使用过Nova的话大家会发现Nova的费用特别低，低至美分级别的gas。其实刚刚潘老师也提到了，在选择DA的时候会考虑到生态的问题，我们Nova链刚上线的第一天已经达到了20多万的地址，我们同时也和Reddit进行了合作，是web2领域顶流的媒体，Arbitrum Nova我们准备的十分充分，欢迎大家来开发体验。谢谢各位！我们进入第四个话题！

### 话题四：

嘉宾对于D/A的理想实现方案是哪种？

### 潘老师：

---

我比较关注Danksharding方案，虽然和celestia共享了DAS、RS纠删码的技术，他们是共同的作者，其实这是一个比较老的技术的新型采用，但是celestia选择的欺诈证明和其他方案有些差别，但是数据可用性采用的很多东西都是类似的，能通过这两个技术降低验证数据可用的成本。同时将节点分为了三类，全节点能提供一个普适的服务，在一定时间内存储数据；归档节点存储了全量的数据，未来以太坊无状态以后，数据对于全节点而言是不用全部存储的，到了一个月或者是一定的时间，数据会丢掉，但是不影响到安全性，但是要回溯历史的话，还需要归档节点来提供历史数据，然后就是轻节点，虽然现在的轻节点可以通过RPC找全节点拿数据，但是以后的验证者节点就可以自己对数据进行采样来确保数据在全网可用。所以我觉得这三类节点的划分也提高了网络中数据可用性的安全保障。

### JasonWan：

---

潘老师可以说一下在实现轻节点的方面有没有技术难点需要开发者共同来解决的？

### 潘致雄：

---

技术难点对我来说比较难评估，周齐可以进行评估一下，但是我看到以太坊基金会官方也在和第三方合作来进行Proto-danksharding和Danksharding的开发，我觉得这里面还是有很多的难点，虽然DAS里面的Erasure Code是CDG时代就有的技术，就是做多倍冗余，在丢失一小部分数据以后仍然能恢复全部数据，十几年前民用级的压缩软件就采用了这些技术，从概念上技术没有那么新，但是应用在区块链上面还是有很多共识上的挑战的，包括结构和网络分发等，所以我觉得工程难度会比较大。

### JasonWan：

---

谢谢潘老师，潘老师认为在工程实现上难度会比较大，那么请周齐老师来聊聊看法。

### QiZhou：

---

对的，刚刚潘老师大致已经把困难和问题介绍清楚了。我来介绍一些大家比较关注的实现难点。

这方面我们和以太坊的核心开发者有面对面的沟通他们在DAS的设计方案、问题以及解决方案。可以看到要解决D/A，以太坊的第一步使用的是EIP-4844，其采用的还是传统的Gossip这样的协议去广播消息，到了接下来的Danksharding以后才会使用DAS和RS纠删码来提高数据冗余和吞吐。这里面还是有很多的技术挑战，举个例子来说，虽然Danksharding极大简化了设计，让分片只关注在数据分片上面，但是对于builder的要求还是非常高的，要求builder在12S的时间内构造一个128M（32*4）的数据矩阵，并且要在12S内将矩阵数据广播到整个网络的所有validator那里，它的广播不是传统的Gossip广播，而是多子网的广播。我问了他们对于builder的优化方案，但是目前还处于一个比较早期的阶段，有一些早期的数据来支撑在硬件（如GPU）和库的优化支持下能够达到这样的一个出块速率。同时也要让所有的验证节点比较快速的去检查所有的样本所对应的数据的正确性，他们在这方面也开发了一些密码学的方案，我看到他们最近在做的是在采样了多个样本的情况下（10个或者100个），只需要multi-prove的计算过程就可以把所有样本的正确性都进行验证。

另外的话，整个网络的架构也是一个不太确定的问题。因为它和我们广泛的比较成熟的区块链的P2P网络的架构不太一样，包括子网的设计，validator的分配，validator在共谋的情况下（藏匿数据）怎么去检测和重构，这里面涉及到很多的网络设计方案和研究话题。

我觉得从短期来说，以太坊EIP-4844从工程角度来说是一个比较容易实现的方案，再接下来，怎么迁移到后面的danksharding，包括一些如EIP-4488这样临时的方案去降低上传的成本，我觉得会有一些新的想法会出现，这是我对以太坊技术路线的观察和思考。

### JasonWan:

---

谢谢周齐老师刚刚从工程的角度为我们解读了目前以太坊D/A实现的困难和潜在的方案，请cyberorange来进行补充一下。

### cyberorange：

---

我觉得理想的D/A要像BitTorrent一样，任何人都可以为网络做贡献，随机存储数据，而且我觉得理想的D/A是不需要出块人的，因为D/A的数据不用管数据的先后顺序和逻辑关联，只需要关注数据是不是可用的，所有我觉得理想的DA是有这样一个网络，当你想让你的某个数据做到D/A的状态的话，你只需要把DA上传到这个网络，你只需要监听你的交易数据有没有被广播，我觉得KZG承诺这些都可以放到客户端。我觉得目前做DA最大的难点就是绝大部分的网络根本没有那么多分散的节点，因为前面做了很多的假设都是有成千上万个独立的分散节点，你把数据都存放到上面，哪怕有50%的节点宕机了，你存储的数据也是有效的。但是事实上，现实情况是很多时候一台电脑就运行了很多的节点，这些节点可能20%在AWS上面，剩下的在某些云服务上面，这种其实是DA的一种很大的风险隐患。因为对于DA来说，任何一个片段的数据丢失了，对于某个应用或者rollup来说都是比较灾难的后果，你认为这个数据是可得到的，但是有一段时间它丢失了。

还有一个比较难的点在于，要做很多节点的D/A，每个节点在为D/A投票的时候要用到签名，签名的验证，聚合广播也是很消耗资源的，在节点很多的情况下，可能签名就把整个网络占据了。

其他就是D/A还带来了一些风险，模块化区块链把风险也模块化了，以前我们只要考虑在Layer1上面是否被审查，现在你首先得考虑在D/A层上面是否被审查，然后是执行层和共识层的审查。整个风险面被放大了。

D/A需要的网络层可能现在都是不一样的，以前都是全广播，现在是采样，使用采样可能对网络层的攻击会增加，比如女巫攻击，日蚀攻击这样的经典的P2P网络攻击方式，所以说，要实现这样的一个网络层还是很难的，我看Polygon Avail和celestia都是直接复用了IPFS的一些网络基础设施，我大概就说这么多。

### JasonWan:

---

感谢cyberorange的解读，其中特别重要的一点就是我们最近常提的审查，新的方案会在抗审查方面带来一些比较困难的点。

### 观众开放问答：

Settlement Layer和Execution Layer到底有什么细微的区别：

### cyberorange：

---

你可以理解为执行在任何一台机器都能执行，比如说你现在在以太坊发起了一笔交易，你可以在你的电脑上面执行这笔交易，但是它是不被这个网络认可的，因为它没有被结算。结算就是你知道一笔交易是不可能篡改，不可能回滚了，这就叫结算；而执行就是字面意义，就是你把这笔交易执行了，你可以在任何地方执行，但是只要没有被结算，就不被整个网络所认可。

### JasonWan：

---

感谢各位嘉宾和听众，今天的AMA到此结束了，希望大家关注各位嘉宾老师进行更多的信息获取，多多关注参与Arbitrum的中文AMA，我们每周都会为大家带来精彩的内容分享。我们下一时间再见!

(感谢@0xCryptoLee的记录！)

[原文链接](https://www.notion.so/941ddb1f60c64ef1a198c4284670de27)